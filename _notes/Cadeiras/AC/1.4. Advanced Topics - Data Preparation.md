---
title: "Advanced Topics - Data Preparation"
---

# Advanced Issues in data  Preparation and Modeling
- Data reduction  
- Dealing with unbalanced classes  
- Final reflections on data quality

## Data Redution
Obtain a reduced representation of the data set that is much smaller in volume  

Producing the same analytical results  or almost the same: 
- improved visualization of data  
- with more interpretable models  
- much faster

### Dimensionality Reduction:  Data is cursed!
- Curse of dimensionality  
	– when dimensionality increases, data becomes  
	increasingly sparse  
	– density and distance between points, which is critical  
	to clustering, outlier analysis, becomes less  
	meaningful  
	– the possible combinations of subspaces will grow  
	exponentially  
- Number of data points required for robust  
patterns grows exponentially with number of  
attributes

### Two Approaches
#### Data Aggregation
##### Using Princiapl Component Analysis (PCA)
– n new features  
– linear combinations of existing n attributes  
– orthogonal to each other  
– k << n explain most of the variance in the data

##### ICA VS PCA
• both create linear combinations of the  attributes  
• ICA assumes the original attributes are statistically independent  
• ... reduces higher order statistics  
	– e.g. kurtosis  
• ... does not rank components

##### Multidimensional Scaling
linear projection of a data set  
• uses the distances between pairs of objects – not the values of the attributes of the objects  
• particularly suitable when it is difficult to extract relevant features to represent the objects

#### Feature Selection
Delete redundant attributes
Delete irrelevant attributes

##### Filter Methods
• 2 attributes  
	– remove redundant attributes  
• 1 attribute vs target  
	– identify relevant variables


## All predicts = negative class
• ML methods usually minimize FP+FN  
	– ... or, at the very least give the same weight to both types of errors  
• ... but potentially FP >> FN  
	– i.e. quality of the model more affected by FP  
• ... so algorithm effectively minimizes FP!  
• ... and there’s an easy model for that  
	– prediction = N

### Data Imbalance
#### collect more data  
Difficult in many domains  

#### resample existing data  
Delete data from the majority class  
Duplicate data from the minority class  

![[over-under-sampling.png]]

#### create synthetic data  
Por exemplo, SMOTE  (Synthetic Minority Over-sampling Technique)
![[smote.png]]

#### Adapt your learning algorithm  
– e.g. cost sensitive learning

## Cost of errors
FP and FN errors often incur different costs  
– medical diagnostic  
– loan decisions  
– marketing campaigns  
– fraud detection in bank transactions  
– fault detection in machines  

But ML methods still usually minimize FP+FN

### Cost-sensite learning
#### Simple Methods  
– Resampling according to costs  
– Weighting according to costs  
	• basically, the same thing  

#### Complex Methods  
##### Metacost
1. Create bootstap replicates of training data  
2. Learn model from each replicate  
3. Relabel examples
4. Learn model on relabelled data

## Data quality: multidimensional view
• accuracy  
	– correct or wrong, accurate or not  
• completeness  
	– not recorded, unavailable, ...  
• consistency  
	– some modified but some not, dangling, ...  
• timeliness  
	– timely update?  
• believability  
	– how trustable is the data and its sources?  
• interpretability  
	– how easily the data can be understood?

### Why worry? Our data is clean!
- we have a data warehouse
- our IS was just revamped
- we had a major data cleanup
- our data is collected automatically
- our data collection is human-error proof
- tell us what you need: we have everything

### How to do better
- human resources
- analytics at the core of IS development
- data quality is a continuous process