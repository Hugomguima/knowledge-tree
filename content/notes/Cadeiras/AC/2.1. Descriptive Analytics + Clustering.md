---
title: "Descriptive Analytics + Clustering"
---

# Descriptive Analytics
#### goals
• Describe/summarize or finding structure on what we have observed  
• Data summarization and visualization (e.g. PCA) can be seen as simple forms of descriptive analytics  
• However, most frequently descriptive modeling is associated with clustering

## Similarity Measures
### How to measure similarity between objects?  
The notion of similarity is strongly related with the notion of  
distance between observations. It can be measured as the oposite of the distance.
Proximity refers to a similarity or dissimilarity.

### Similarity measure  
• Numerical measure of how alike two data objects are.  
• Is higher when objects are more alike.  
• Often falls in the range [0, 1]

### Dissimilarity measure  
• Numerical measure of how different two data objects are  
• Lower when objects are more alike  
• Minimum dissimilarity is often 0 Upper limit varies

Dissimilarity measure can be expressed by a distance metric. 
Distance metrics d have some well-known properties.

#### Euclidian Distance
#### Manhattan Distance
#### Minkowski Distance

More examples:
- Canberra distance  
- Jaccard Coefficients  
- Cosine similarity

Still, several problems may arise that may distort the notion of  
distance:  
• different scales of variables  
• different importance of variables  
• different types of data (e.g. both numeric and categorical variables)

#### Heterogeneous Distance Functions
#### General Coefficient of Similarity

# Clustering
#### Goals
Obtain the “natural” grouping of a set of data - i.e. find some structure on the data set  
	• The key issue on clustering is the notion of similarity  
	• Observations on the same group are supposed to share some properties, i.e. being similar  
	• Most methods use the information on the distances among observations in a data set to decide on the natural groupings of the cases  
Provide some abstraction of the found groups (e.g. a representation of their main features; a prototype for each group; etc.), gain novel insights of data

#### Applications
**Biology** 
	• describe spatial and temporal communities of organisms  
	• group genes or proteins that have similar functionality  
**Business and Marketing**  
	• describe different market segments from a set of potetential clients  
	• group stocks with similar price fluctuations  
**Web Mining**  
	• find groups of related documents for information retrieval  
	• find communities in social networks  
	• build recommender systems

### Main Types of Clustering
Partitional
	- Divide the observations in k partitions according to some criterion  

Hierarchical
	- generate a hierarchy of groups, from 1 to n groups, where n is the number of lines in the data set  
	
Agglomerative
	- generate a hierarchy from bottom to top (from n to 1 group)
  
Divisive
	- Create a hierarchy in a top down way (from 1 to n groups)

#### Clustering Partitional Methods
Goal: Partition the given set of data into k groups by either minimizing  
or maximizing a pre-specified criterion  

Some key issues:  
	• The user needs to select the number of groups  
	• The number of possible divisions of n cases into k groups can grow fast!

Some important properties  
	• Cluster compactness  
		• how similar are cases within the same cluster  
	• Cluster separation  
		• how far is the cluster from the other clusters  
	• The goal is to minimize intra-cluster distance and maximize inter-cluster distances.  
	• A clustering solution assigns all the objects to a cluster  
		• hard clustering: an object belongs to a single cluster  
		• fuzzy clustering: each object has a probability associated to belong  to each cluster

##### k-Means
It is a partition-based method that obtains k groups of a data set k-means algorithm.
• Initialize the centers of the k groups to a set of randomly chosen observations  

• Repeat  
	• Allocate each observation to the group whose center is nearest  
	• Re-calculate the center of each group  
	
• Until the groups are stable, i.e. there is no significant decrease or there is an increase on the minimize criterion h(C)

Some observations:  
	• It uses the squared Euclidean distance as criterion  
	• Maximizes inter-cluster dissimilarity  
Advantages:  
	• Fast algorithm that scales well  
	• Stochastic approach that frequently works well. It tends to identify local minima.  
Disadvantages:  
	• It does not ensure an optimal clustering  
	• We may obtain different solutions with different starting points  
	• The initial guess of k for the number of clusters, maybe away from the real optimal value of k .

	
### Clutering Validation
How to validate/evaluate/compare the results obtained by some  
clustering method?  
	• Is the found group structure random?  
	• What is the “correct” number of groups?  
	• How to evaluate the result of a clustering algorithm when we do  
	not have information on the number of groups in the data set?  
	• How to compare the results obtained by different methods when  
	outside information on the number of groups exists?  
	• How to compare alternative solutions (e.g. obtained using different  
	clustering algorithms)

#### Supervised 
Compare the obtained clustering (grouping) with the  
external information that we have available

#### Unsupervised
Try to measure the quality of the clustering without  
any information on the “ideal” structure of the data.
- Cohesion coefficients - determine how compacts/cohesive are the  
members of a group  
- Separation coefficients - determine how different are the members  
of different groups

#### Silhouette Coefficient
Popular coefficient that incorporates both the notions of cohesion  
and separation. The coefficient takes values between −1 and 1.

Example: iris data set silhouette coefficients si with k = 3 clusters  
• Large si (almost 1) means that they are very well clustered.  
• Small si (around 0) means that they lie between two clusters.  
• Negative si means that they are probably placed in the wrong cluster.  
• The closer average silhouette to 1, the better.

### Best Number of Clusters
• An inappropriate choice of k can result in a clustering with poor  
performance.  
• What happens if we select a k that is too high? What if the k is too low?  
• Ideally, you should have some a priori knowledge on the real structure of the data.  
• If no a priori value is known start with √n/2 as a rule of thumb,  
where n is the number of attributes.

For several possible number of clusters k:  
• Calculate the average silhouette coefficient value and choose the  
k that yields to the highest value

#### Elbow Method
Calculate the within-cluster SSE, also called distortion, and  
choose the k so that adding another cluster doesn’t yield to a  
much smaller SSE


### Other Clustering Partitional Methods
#### PAM (Partitioning Around Medoids)  
• It searches for the k representative objects (the medoids) among  
the cases in the given data set.  
• As with k-means each observation is allocated to the nearest  
medoid.  
• Is more robust to the presence of outliers because it uses original  
objects as centroids instead of averages that may be subject to  
the effects of outliers.  
• Moreover, it uses a more robust measure of the clustering quality:  
L1 − norm, which is based on absolute error instead of the  
squared error used in k-means

#### CLARA (Clustering Large Applications)  
The PAM algorithm has several advantages in terms of robustness  
when compared to k-means.  

However, these advantages come at the price of aditional  
computational complexity that may be too much for very large data  
sets.
• CLARA tries to solve these efficiency problems  
• It does that by using sampling, i.e. working on parts of the data set  
instead of the full data set

##### Algorithm
Repeat n times the following:  
	• Draw a random sample of size m  
	• Apply PAM to this random sample to obtain k centroids  
	• Allocate the full set of observations to one of these centroids  
	• Calculate sum of dissimilarities of the resulting clustering (as in PAM)  
Return as result the clustering of the n repetitions that got lowest sum of dissimilarities

#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  
• The density of a single observation is estimated by the number of  
observations that are within a certain radius (a parameter of the  
method)  
• Based on this idea observations are classified as:  
• core points: if the number of observations within its radius are above  
a certain threshold  
• border points: if the number of observations within their radius does  
not reach the threshold but they are within the radius of a core point  
• noise points: they do not have enough observations within their  
radius, nor are they sufficiently close to any core point

##### DBSCAN Algorithm  
• Classify each observation in one of the three possible alternatives  
• Eliminate the noise points from the formation of the groups  
• All core points that are within a certain distance of each other are  
allocated to the same group  
• Each border point is allocated to the group of the nearest core point  

Note that this method does not require the user to specify the  
number of groups.  
But, you need to specify the radius (ε) and the minimum number  
of points (MinPts)

##### Advantages
• Can handle clusters with different shapes and sizes  
• Resistant to noise  

##### Disadvantages
• Varying densities  
• High-dimensional data

## Hierarchical Clustering
#### Goals
• Obtain a hierarchy of groups, where each level represents a possible solution with x groups. It is up to the user to select the solution he wants.  
• A dendogram can be used for visualization

#### Agglomerative Methods - bottom-up  
• Start with as many groups as there are cases  
• On each upper level a pair of groups is merged into a single group  
• The chosen pair is formed by the groups that are more similar 

#### Divisive Methods - top-down (much less used)  
• Start with a single group  
• On each level select a group to be split in two  
• The selected group is the one with smallest uniformity

Some proximity measures for the merging/splitting step  
- single link  
- complete link  
- average link  

Other methods also exist (e.g. distance between the centroids, Ward’s  
method that uses SSE).

### Agglomerative methods
#### Algorithm  
• Compute the proximity matrix  
• Let each data point be a cluster  
• Repeat  
	• Merge the two closest clusters  
	• Update the proximity matrix  
• Until only a single cluster remains

#### Different proximity measures yield to different types of clusters
Single-link
	• can handle non-elliptical shapes  
	• uses a local merge citerion  
	• distant parts of the cluster and the clusters’ overall structure are not taken into account

Complete-link  
	• biased towards globular clusters  
	• uses a non-local merge citerion  
	• chooses the pair of clusters whose merge has the smallest diameter  
	• the similarity of two clusters is the similarity of their most dissimilar members  
	• sensitive to noise/outliers

 Average-link  
	• it is a compromise between single and complete link

### Divisive Methods
#### Algorithm  
• Compute the proximity matrix  
• Start with a single cluster that contains all data points  
• Repeat  
	• choose the cluster with the largest diameter, i.e. largest dissimilarity between any two of its points  
	• select the data point with largest average dissimilarity to the other  
	members in that cluster  
	• re-allocate the data points to either the cluster of this selected point or the “old” cluster (represented by its center), depending on which one is nearest  
• Until each data point constitutes a cluster

## Wrap Up
We can compare clustering methods

#### Algorithm
• complexity and scalability  
• similarity measures that can be employed  
• robustness to noise  
• it is able to find clusters on sub-spaces  
• different runs lead to different results  
• it is incremental

#### Data
• it is able to handle different types of data (continuous, categorical,  
binary)?  
• is there dependency on the order of data points?  

#### Domain
• does the algorithm finds the number of clusters, or needs it as input?  
• how many parameters are necessary?  
• what is the required domain knowledge for that?  

#### Results
• shape of clusters that is able to find  
• interpretability