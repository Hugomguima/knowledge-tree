---
title: "Data Understanding"
---

### CRISP-DM: Data Understanding
- Collect Initial Data:  
	initial data collection report  
- Describe Data:  
	data description report  
- Explore Data:  
	data exploration report  
- Verify Data Quality:  
	data quality report

### CRISP-DM: Data Preparation
- Data Set:  
	data set description  
- Select Data:  
	rationale for inclusion/exclusion  
 - Clean Data:  
	data cleaning report  
 - Construct Data:  
	derived variables, generated records  
- Integrate Data:  
	merged data  
- Format Data:  
	reformatted data

## Data Undersanting - Summarization
**Motivation**  
	• With big data sets it is hard to have an idea of what is going on in  
	the data  
	• Data summaries provide overviews of key properties of the data  
	• Help selecting the most suitable tool for the analysis  
	• Their goal is to describe important properties of the distribution of  
	the values  
**Types of Summaries**
	• What is the “most common value”?  
	• What is the “variability” in the values?  
	• Are there “strange” / unexpected values in the data set?

- Data set  
	• Univariate data  
	• Multivariate data  
- Variables  
	• Categorical variables  
	• Numeric variables

- Example Data set  
	• algae data set composed by 200 water samples taken at several  
	European rivers, which are described by:  
	• 3 categorical variables: season, size and speed of the river  
	• 8 numeric variables with chemical concentration measurements  
	• 7 numeric variables with the concentration level of harmful algae.
	  
• Mode: the most frequent value  
• Frequency table: frequency of each value (absolute or relative)
• Contingency tables: cross-frequency of values for two variables

### Numeric Variables
Statistics of location  
• Mean (or sample mean) - sensitive to extreme values  
• Median  
	• It is the 50th-precentile, i.e. the value above (below) which there are 50% of the values in the data set  
• Mode  
	• It is the most common (more frequently occurring) value in a set of values  
• Note that the mode can be applied to categorical variables

#### Statistics of variability or dispersion  
• Range: max − min
• Variance  - sensitive to extreme values  
• Standard Deviation - sensitive to extreme values  
• Inter-quartile Range (IQR)

#### Multivariate analysis of variability or dispersion
• Covariance Matrix: variance between every pair of numeric variables - the value depends on the magnitude of the variable
• Correlation Matrix: correlation between every pair of numeric variables - the influence of the magnitude is removed

Pearson Correlation Coefficient (ρ):
	• measures the linear correlation between two variables;
	• it has a value between +1 and -1
	
Spearman Rank-Order Correlation Coefficient:
	• measures the strength and direction of monotonic association between two variables;  
	• two variables can be related according to a type of non-linear but still monotonic relationship.
	• a rank-based, and non-parametric, version of Pearson correlation coefficient;  
	• it has a value between +1 and -1;

## Data Visualization
### Univariate Graphs
**Barplots**

**Piecharts**

**Line Plots**

**Histograms**

**Cumulative Distribution Function (CDF)**

**QQ Plots**

**Boxplots**

### Bivariate Graph
**Scatterplots**
The natural graph for showing the relationship between two  numeric variables. However, scatterplot can plot the relationship between every pair of  numeric variables and respective groups, being a *Multivariate Graph*.

### Multivariate Graph
**Parallel Coordinates Plot**

**Correlogram**

**Conditioned Graphs**
Data sets frequently have categorical variables, which values can be used to create sub-groups of the data.  
	Example: The sub-group of male clients of a company  
	
Conditioned plots allow the simultaneous presentation of these sub-group graphs to better allow finding eventual differences between the sub-groups.
Some examples are Conditioned Histograms and Conditioned Boxplots.

## Data Preparation
Set of steps that may be necessary to carry out before any further  
analysis takes place on the available data.  
• Data can come from a multitude of different sources  
• Frequently, we have data sets with unknown variable values  
• Many data mining methods are sensitive to the scale and/or the  
type of variables  
• Different variables may have different scales  
• Some methods are unable to handle either nominal or numerical  
variables
• We may face the need to “create” new variables to achieve our  
objectives  
• Sometimes we are more interested in relative values (variations)  
than absolute values  
• We may be aware of some domain-specific mathematical  
relationship among two or more variables that is important for the  
task  
• Our data set may be too large for some methods to be applicable

• Feature Extraction  
• extract features from raw data on which analysis can be performed.  
• Data Cleaning  
• data may be hard to read or require extra parsing efforts.  
• Data Transformation  
• it may be necessary to change some of the values of the data.  
• Feature Engineering  
• to incorporate some domain knowledge.  
• Data and Dimensionality Reduction  
• to make modeling possible.

## Feature Extraction
• It is very application specific and a very crucial step.  
• sensor data: large volume of low-level signals associated with  
date/time attributes  
• image data: very high-dimensional data that cane be represented  
by pixels, color histograms, etc.  
• web logs: text in a prespecified format with both categorical and  
numerical attributes  
• network traffic: network packets information  
• document data: raw and unstructured data

## Data Cleaning
### Goal
• Making our data set tidy  
• each value belongs to a variable and an observation  
• each variable contains all values of a certain property measured  
across all observations  
• each observation contains all values of the variables measured for  
the respective case  
• These properties lead to data tables where:  
• each row represents an observation  
• each column represents an attribute measured for each observation

### Main Strategies  
• Remove all cases in a data set with some unknown value  
• Fill-in the unknowns with the imputation of the most common  
value (a statistic of centrality)  
• Fill-in with the most common value on the cases that are more  
“similar” to the one with unknowns.  
• Fill-in with linear interpolation of nearby values in time and/or  
space.  
• Explore eventual correlations between variables  
• Do nothing: many data mining methods are designated to work  
robustly with missing values

• Inconsistency detection  
• data integration techniques within the database field  
• Domain knowledge  
• data auditing that use domain knowledhe and constraints  
• Data-centric methods  
• statistical-based methods to detect outliers

## Data Transformation
• Map entire set of values of a given attribute to a new set of  
replacement values such that each old value can be identified with  
one of the new values  
• Why it may be useful?  
• Imagine two attributes (e.g. age and salary) with a very different  
scale  
• Any aggregation function (e.g. euclidean distance) computed on the  
set of cases, will be dominated by the attribute of larger magnitude.  

### Some common strategies:  
#### Normalization  
Min-Max Scaling (Range-based Normalization)
Standarization (z-score Normalization)

#### Case Dependencies
In time series it is common to use different techniques.  
Examples:  
	• to adjust mean, variance, range  
	• to remove unwnated, common signal

#### Binarization / One-Hot Enconding  
• Some data mining methods are only able to handle numeric attributes.  
• If the categorical attribute is not ordinal, it is necessary to convert it into a numerical attribute.  
• Binarization: if the atribute has only 2 possible nominal values, it can be transformed into 1 binary attribute  
• fever: yes/no − > fever: 1/0  
• One-Hot Enconding: if the atribute has k possible nominal values, it can be transformed into k binary attributes  
• eye_color: brown/blue/green → eye_brown: 1/0, eye_blue: 1/0,  
eye_green: 1/0

#### Discretization
• Process of converting a continuous attribute into an ordinal  
attribute of numeric variables.  
• Some unsupervised discretization: find breaks in the data values  
• Equal-width  
• it divides the original values into equal-width range of values  
• it may be affected by the presence of outliers  
• Equal-frequency  
• it divides the original values so that the same number of values are  
assigned to each range  
• it can generate ranges with very different amplitudes  
• Supervised discretization: use class labels to find breaks (we’ll  
see later)

## Feature Engineering
Fundamental to the application of machine learning.  
’(...) some machine learning projects succeed and some fail. What makes the  
difference? Easily the most important factor is the features used.’ - Pedro  
Domingos, 2012  
• The process of using domain knowledge of the data to create  
features that might help when solving the problem.  
• New features that can capture the important information in a data  
set much more efficiently than the original features.  
• Case 1: express known relationships between existing variables  
• create ratios and proportions like credit card sales per person  
• from web logs obtain the average session duration per user, the  
frequency of access, etc.

• Case 2: overcome limitations of some data mining tools regarding  
cases dependencies.  
• some tools shuffle the cases, or are not able to use the information  
about their dependencies (time, space, space-time)  
• two main ways of handling this issue:  
• constrain ourselves to tools that handle these dependencies directly  
• create variables that express the dependency relationships  
• In time series is common to create features that represent relative  
values instead of absolute values, so to avoid trend effects.

• Other common technique is Time Delay Embedding  
• Create variables whose values are the value of the same variable in previous time steps  
• If we have variables whose values are the value of the same variable but on different time steps, standard tools will be able to model the time relationships with these embeddings 
• Note that similar “tricks” can be done with space and space-time  
dependencies