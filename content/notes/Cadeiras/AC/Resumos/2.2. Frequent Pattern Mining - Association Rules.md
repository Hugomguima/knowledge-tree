---
title: "Frequent Pattern Mining"
---

# Association Rules
Originally stated in the context of *Market Basket Analysis*

- Data consists of set of items bought by costumers, referred as transactions  
- Find unexpected associations between sets of items using the frequency of sets of items  
- Discovered sets of items are referred as frequent itemsets or frequent patterns  

## Actionable Knowledge
### Shop Layout
Actions for rule {A1, A4} → {A6}  
- Sell the A1, A4, A6 together (pack)  
- Place article A6 next to articles A1, A4  
- Offer a discount coupon for A6 in articles A1, A4  
- Place a competitor of A6 next to A1, A4 (brand protection).  
  
These actions must make sense from the business point of view.

### Cross Selling
Following these steps:
1. Client puts article A in basket  
2. Shop knows rule A → B  
3. Rule has enough confidence (> 20%)  
4. Shop tells client he may be interested in B  
5. Client decides whether to buy B or not

### Text Mining
Each document is treated as a “bag” of terms and keywords, with the goal to identify co-occurring terms and keywords

### Health
Rules obtained from the patient’s records.  A set of observations may fire a rule.
For example: {Head ache, blood pressure rise} → {stroke, immobilization}
Not necessarily causal

### Web Usage Analysis
Usage patterns: 
- Most visited pages  
- Frequent page sets  
- Pages associated to users  
- Seasonal effects  
- Cross-preferences

## Basic Concepts
 ### Market Basket Analysis
 -> Convert products inside a basket case to a binary flag
 
![[binary-flags.png]]

### Problems
**How frequent is an itemset?**
 Imagine that Sugar, Flower and Eggs are sold together
 
 *Support* measures the importance of a set.
	 -  Percentage of transactions t containing the set S
	 -  Absolute support: number of transactions t containing the set S

**How predictive is an itemset?**
Frequent itemsets are used to generate association rules.  
If you buy sugar and flower, you also buy eggs.

*Confidence* measures the strength of the rule.

### Generalization
TODO

## Mining Association Rules
### Problem Definition
Given:  
- data set of transactions D  
- minimal *Support* minsup  
- minimal *Confidence* minconf  
 
Obtain all association rules:  
	X → Y (s = Sup, c = Conf )  
		such that:  
	Sup ≥ minsup and Conf ≥ minconf

### Apriori Algorithm
The Apriori Algorithm [Agrawal and Srikant, 1994] works in two steps:
1. Frequent itemset generation  
	• itemsets with support ≥ minsup  
1. Rule generation  
	• generate all confident association rules from the frequent itemsets, i.e. rules with confidence ≥ minconf

#### Problem:  
- there is a very large number of candidate frequent itemsets! 

#### Downward Closure Property  
- every subset of a frequent itemset must also be frequent.  
- thus, every superset of an infrequent itemset is also infrequent.  

#### Apriori Pruning Principle:  
- if an itemset is below the minimal support, discard all its supersets.

**Step 1 - Identifying Frequent Itemsets**
- Candidate generation (Self-Join step)  
- Candidate pruning (Prune step)

**Step 2 - Rule Generation**
- generates all non-empty subsets s of each frequent itemset I  
- for each subset s computes the confidence of the rule (I − s) → s  
- selects the rules whose confidence is higher than minconf

#### Complexity factors
Number of items  
Number of transactions  
Minimal support  
Average size of transactions  
Number of frequent sets  
Average size of a frequent size  
Number of DB scans  

#### Compact Representation of Itemsets
The number of frequent itemsets produced from a transaction data set can be very large.  

It is useful to identify a small representative set of itemsets from  
which all other frequent itemsets can be derived.  
Two such representations are:  
- maximal  
- closed

### Too many rules
The association rule algorithms tend to generate an excessive number of rules (for some problems, there can be thousands).  
• Too many rules leads to model’s interpretability lack.  
• How can we reduce this number?  
• Changing the parameters: minsup, minconf  
• Restrictions on items: which items are relevant?  
• Summarization techniques: can we represent subsets of rules by a  
single representative rule?  
• Filter rules: improvement, measures of interest

### Mesure the interest of a rule
**Subjective measures**: based on user’s belief in the data (ex:  
unexpectedness, novelty, actionability, confirm hypothesis user  
wishes to validate)  
	• These measures are hard to incorporate in the pattern discovery  
	task.  

**Objective measures**: based on facts, statistics and structures of  
patterns (ex: support and confidence), independent of the  
domain considered.  
	• For instance, patterns that involve mutually independent items or  
	cover very few transactions are considered uninteresting

#### Lift
Is the ratio between confidence of the rule and the support of  
the itemset appearing in the consequent

### Improving Apriori
Challenges of Frequent Pattern Mining  
• Multiple scans of transaction database  
• Huge number of candidates  
• Tedious workload of support counting for candidates  

Improving Apriori: general ideas
• Reduce number of transaction database scans  
• Shrink number of candidates (bottleneck of Apriori)  
• Facilitate support counting of candidates  

Some methods that improve Apriori’s efficiency:  
• Partitioning [Savasere et al., 1995]  
• Sampling [Toivonen, 1996]  
• Dynamic Itemset Counting [Brin et al., 1997]  
• Frequent Pattern Projection and Growth (FP-Growth)

## Conclusions
Association rule mining:  
• Frequent itemsets (requires min support)  
• Association rules (requires min confidence)  
• Probabilistic implications  

One of the most used data mining tools  
• Problem: generates too much rules  
• Pattern compression and pattern selection  

Several algorithms:  
• Apriori is the most known algorithm  
• There are variants of Apriori that return exactly the same patterns!  
• Completeness: find all rules.